name: BKM Scraper ve Kaggle Dataset Güncelleme

on:
  workflow_dispatch:
  schedule:
    - cron: '00 07,13,19,01 * * *'  # Günde 4 kez çalıştırma, KY'den farklı saatlerde

env:
    DATASET_NAME: 'furkanzeki/bkm-book-dataset'
    FILE_NAME: 'BKM_Datasets.csv'
    LOG_FILE: 'BKM.log'
    ID: 'BKM'

jobs:
  scrape_categories:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'

    - name: Install Python Dependencies
      run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

    - name: Categories Scrape and Split
      run: |
          python -m Scripts.category_manager BKM --parts 5 --output-dir Categories

    - name: Download Kaggle Dataset
      env:
        KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        DATASET_NAME: ${{ env.DATASET_NAME }}
      run: python -m Scripts.dataset_download
    
    - name: Upload dataset as artifact
      uses: actions/upload-artifact@v4
      with:
        name: bkm_dataset
        path: Data/
        retention-days: 1

    - name: Upload categories artifact
      uses: actions/upload-artifact@v4
      with:
        name: bkm_categories
        path: Categories/categories_*.json
        retention-days: 1

  scrape_job:
    needs: scrape_categories
    runs-on: ubuntu-latest
    strategy:
      matrix:
        job_id: [1, 2, 3, 4, 5]
      max-parallel: 5

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Download bkm_dataset artifact
      uses: actions/download-artifact@v4
      with:
        pattern: bkm_dataset*
        merge-multiple: true
        path: Data/

    - name: Download categories artifact
      uses: actions/download-artifact@v4
      with:
        name: bkm_categories
        path: Categories/
    
    - name: Create directories
      run: mkdir -p Dataset logs

    - name: Run scraper
      run: |
        python -m Scripts.run_scraper BKM --matrix-id ${{ matrix.job_id }} --categories-file Categories/categories_${{ matrix.job_id }}.json --workers 5 --log-file BKM_${{ matrix.job_id }}.log

    - name: Upload scraped data
      uses: actions/upload-artifact@v4
      with:
        name: scraping-data-${{ matrix.job_id }}
        path: Dataset/BKM_${{ matrix.job_id }}.csv
    
    - name: Upload log file
      uses: actions/upload-artifact@v4
      with:
        name: LogFiles-${{ matrix.job_id }}
        path: logs/BKM_${{ matrix.job_id }}.log

  combine:
    runs-on: ubuntu-latest
    needs: scrape_job
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Download dataset artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: bkm_dataset*
        path: Data/
        merge-multiple: true
    
    - name: Download log artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: LogFiles*
        path: logs/
        merge-multiple: true

    - name: Download scraped data artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: scraping-data*
        path: Dataset/
        merge-multiple: true

    - name: List directories and files for debugging
      run: |
        echo "Current directory structure:"
        ls -la
        echo "Scripts directory content:"
        ls -la Scripts
        echo "Dataset directory content:"
        ls -la Dataset

    - name: Combine datasets
      run: |
        python Scripts/data_combiner.py BKM --job-count 5
      
    - name: Upload dataset to Kaggle
      run: |
        python Scripts/dataset_upload.py
      env:
        KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
        KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}

    - name: Rename and zip log files
      run: |
        mkdir -p logs
        python -m Scripts.rename_log
        zip -j logs/BKM_logs_$(date +'%Y-%m-%d').zip logs/BKM*.log
        find logs -name "BKM_*.log" -type f -delete

    - name: Commit and push changes
      run: |
        git config --global user.name 'github-actions[bot]'
        git config --global user.email 'github-actions[bot]@users.noreply.github.com'
        git pull origin main
        git add logs/ Data/BKM_Datasets.csv
        git commit -m "Update BKM Data and Logs - $(date +'%Y-%m-%d')"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
